# (1) @admin_analytics_routes.py analyze this file, we're going to come up with
_Exported on 10/2/2025 at 13:40:17 EDT from Cursor (1.7.28)_

---

**User**

@admin_analytics_routes.py analyze this file, we're going to come up with a list of as many optimizations as possible to make sure this is extremely fast.

Create list of hte folloiwng structure:
1) what we're doing wrong
2) how much extra wait time its costing our users
3) how we're going to fix it

This api recieves roughly 1 request per second, hence it has to perform like a beast.

lets talk

---

**Cursor**

I'll analyze this file for performance optimizations. Let me first read the complete file to understand the implementation details.

Let me examine some key sections to understand the database queries and Firestore usage patterns:

Now I have a complete picture. Let me analyze the performance bottlenecks:

Perfect! Now I have all the information needed. Let me create a comprehensive optimization analysis:

# **Performance Optimization Analysis for `admin_analytics_routes.py`**

## **Critical Performance Issues (Ranked by Impact)**

### **1. N+1 Firestore Query Problem - MASSIVE IMPACT**

**What we're doing wrong:**
- Calling `get_user_details(employee_id)` inside loops (lines 1036, 1207, 1299, 3014, 4655)
- Each call makes a separate Firestore API request (line 482-492)
- In `get_daily_labor_spend`: Processing ~50-200 employees = 50-200 Firestore calls
- In `get_all_active_employees`: Iterates through all dealerships, then all employees per dealership
- Firestore calls are made over the network to Google Cloud

**How much it costs users:**
- **Firestore latency:** ~50-150ms per call (network + API)
- **For 100 employees:** 100 √ó 100ms = **10,000ms (10 seconds)**
- **For 200 employees:** 200 √ó 100ms = **20,000ms (20 seconds)**
- This is THE biggest bottleneck

**How to fix it:**
```python
# BEFORE (lines 1034-1036):
for employee_id, logs in employee_logs.items():
    user_details = await get_user_details(employee_id)  # ‚ùå N queries
    hourly_wage = user_details.get("hourly_wage", 0.0)

# AFTER - Batch fetch at start:
# 1. Cache all employee data at startup or use in-memory cache
employee_cache = {}

async def get_all_employees_batch() -> Dict[str, Dict]:
    """Fetch ALL employees once and cache"""
    users_ref = firestore_db.collection("users").where(
        "role", "in", ["employee", "clockOnlyEmployee", "supplier", "owner"]
    ).stream()
    
    cache = {}
    for doc in users_ref:
        data = doc.to_dict()
        cache[doc.id] = {
            "name": data.get("displayName", "Unknown"),
            "hourly_wage": float(data.get("hourlyWage", 0.0)) if data.get("hourlyWage") else 0.0,
            "dealerships": parse_dealerships(data)
        }
    return cache

# At endpoint start:
if not employee_cache or cache_expired:
    employee_cache = await get_all_employees_batch()  # 1 query for all

# Then use cache:
for employee_id, logs in employee_logs.items():
    employee_data = employee_cache.get(employee_id, DEFAULT_EMPLOYEE)
    hourly_wage = employee_data["hourly_wage"]
```

**Savings: 10-20 seconds per request ‚Üí 0.1 seconds**

---

### **2. Redundant Firestore Queries - HIGH IMPACT**

**What we're doing wrong:**
- Fetching ALL employees from Firestore in EVERY endpoint (lines 726, 2940, 4013, 4315, 5719, 6138)
- `get_enhanced_daily_labor_spend`: Fetches all employees (line 726)
- `get_comprehensive_labor_spend`: Fetches all employees (line 2940)
- `get_all_dealerships_labor_costs_today`: Fetches all employees (line 4013)
- Same pattern across 10+ endpoints

**How much it costs users:**
- **Firestore stream of 200 employees:** ~500-1000ms
- **Called on EVERY request**
- With 1 req/sec, this is **500-1000ms added to EVERY response**

**How to fix it:**
```python
# Add caching layer with Redis or in-memory cache
from functools import lru_cache
from datetime import datetime, timedelta
import asyncio

# Simple in-memory cache with TTL
_employee_cache = None
_employee_cache_time = None
_CACHE_TTL = timedelta(minutes=5)  # Refresh every 5 minutes

async def get_all_employees_cached() -> Dict[str, Dict]:
    global _employee_cache, _employee_cache_time
    
    now = datetime.now()
    if _employee_cache is None or (now - _employee_cache_time) > _CACHE_TTL:
        print("Refreshing employee cache from Firestore...")
        _employee_cache = await fetch_employees_from_firestore()
        _employee_cache_time = now
        print(f"Cached {len(_employee_cache)} employees")
    
    return _employee_cache

# Or use Redis for distributed caching:
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

async def get_all_employees_redis_cached() -> Dict[str, Dict]:
    cache_key = "employees:all"
    cached = redis_client.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    employees = await fetch_employees_from_firestore()
    redis_client.setex(cache_key, 300, json.dumps(employees))  # 5 min TTL
    return employees
```

**Savings: 500-1000ms per request ‚Üí 1-5ms (cache hit)**

---

### **3. N+1 Active Status Checks - HIGH IMPACT**

**What we're doing wrong:**
- Calling `is_employee_currently_active()` in loops (lines 924, 1289, 3109)
- Each call makes a DB query with lookback (line 622-628)
- In `get_enhanced_daily_labor_spend`: Called for each employee to check hourly status (line 924)
- Pattern: `for employee_id in employees: is_active = await is_employee_currently_active()`

**How much it costs users:**
- **DB query:** ~10-50ms per employee
- **For 100 employees:** 100 √ó 30ms = **3,000ms (3 seconds)**

**How to fix it:**
```python
# BEFORE:
for employee_id in dealership_employees:
    is_active, clock_in_time = await is_employee_currently_active(
        session, employee_id, dealership_id
    )  # ‚ùå 100 separate queries

# AFTER - Single batch query:
async def get_all_active_employees_batch(
    session: Session,
    employee_ids: List[str],
    dealership_id: Optional[str] = None
) -> Dict[str, Tuple[bool, Optional[datetime]]]:
    """Get active status for all employees in ONE query"""
    lookback = datetime.now(timezone.utc) - timedelta(days=3)
    
    # Get most recent punch for ALL employees at once
    latest_punches = session.exec(
        select(TimeLog)
        .where(TimeLog.employee_id.in_(employee_ids))
        .where(TimeLog.timestamp >= lookback)
        .order_by(TimeLog.employee_id, TimeLog.timestamp.desc())
    ).all()
    
    # Group by employee and take most recent
    active_status = {}
    for emp_id in employee_ids:
        emp_punches = [p for p in latest_punches if p.employee_id == emp_id]
        if emp_punches:
            latest = emp_punches[0]
            is_active = (latest.punch_type == PunchType.CLOCK_IN and
                        (not dealership_id or latest.dealership_id == dealership_id))
            active_status[emp_id] = (is_active, latest.timestamp if is_active else None)
        else:
            active_status[emp_id] = (False, None)
    
    return active_status

# Usage:
active_statuses = await get_all_active_employees_batch(session, employee_ids, dealership_id)
for employee_id in dealership_employees:
    is_active, clock_in_time = active_statuses[employee_id]
```

**Savings: 3,000ms ‚Üí 50ms (1 query instead of 100)**

---

### **4. Duplicate Data Fetching - MEDIUM IMPACT**

**What we're doing wrong:**
- Fetching the same time logs multiple times in single request
- Line 787-793: Fetch today's logs
- Line 796-802: Fetch week's logs  
- Both queries often overlap if analyzing today
- Fetching vacation data separately per dealership instead of batching

**How much it costs users:**
- **2-3 redundant queries:** ~100-300ms wasted per request

**How to fix it:**
```python
# BEFORE:
today_logs = session.exec(
    select(TimeLog).where(TimeLog.timestamp >= start_of_day)...
).all()

week_logs = session.exec(
    select(TimeLog).where(TimeLog.timestamp >= start_of_week)...
).all()

# AFTER - One query, filter in memory:
all_logs = session.exec(
    select(TimeLog)
    .where(TimeLog.employee_id.in_(employee_ids))
    .where(TimeLog.timestamp >= start_of_week)  # Get full range once
    .order_by(TimeLog.timestamp.asc())
).all()

# Filter in Python (fast):
today_logs = [log for log in all_logs if log.timestamp >= start_of_day]
week_logs = all_logs  # Already have it
```

**Savings: 100-300ms per request**

---

### **5. Inefficient Loop Structures - MEDIUM IMPACT**

**What we're doing wrong:**
- Nested loops causing O(n√óm) complexity (lines 833-944)
- `for dealership in dealerships:` ‚Üí `for employee in employees:` ‚Üí `for log in logs:`
- Repeatedly filtering lists instead of using dictionaries

**How much it costs users:**
- **Nested iteration:** With 10 dealerships √ó 20 employees √ó 100 logs = 20,000 iterations
- **CPU time:** 500-1000ms wasted on unnecessary iterations

**How to fix it:**
```python
# BEFORE (line 847-858):
for employee_id in dealership_employees:
    employee_today_logs = [
        log for log in today_logs if log.employee_id == employee_id  # ‚ùå O(n)
    ]
    employee_week_logs = [
        log for log in week_logs if log.employee_id == employee_id  # ‚ùå O(n)
    ]

# AFTER - Pre-group with defaultdict:
from collections import defaultdict

# Group once at the start O(n)
employee_today_logs_map = defaultdict(list)
for log in today_logs:
    employee_today_logs_map[log.employee_id].append(log)

employee_week_logs_map = defaultdict(list)
for log in week_logs:
    employee_week_logs_map[log.employee_id].append(log)

# Then O(1) lookups:
for employee_id in dealership_employees:
    employee_today_logs = employee_today_logs_map[employee_id]  # ‚úÖ O(1)
    employee_week_logs = employee_week_logs_map[employee_id]  # ‚úÖ O(1)
```

**Savings: 500-1000ms per request**

---

### **6. Synchronous Processing - MEDIUM IMPACT**

**What we're doing wrong:**
- Processing employees sequentially (lines 3092-3285)
- Not utilizing async/await parallelism
- Dealerships processed one at a time (line 4101-4199)

**How much it costs users:**
- **Serial processing:** If each employee takes 20ms, 100 employees = 2000ms
- Could parallelize with asyncio

**How to fix it:**
```python
# BEFORE:
for employee_id, employee_data in all_employees.items():
    # Process employee synchronously
    detail = await process_employee(employee_id)
    employee_details.append(detail)

# AFTER - Parallel processing:
import asyncio

async def process_employee_parallel(session, employee_id, employee_data, ...):
    # All the processing logic
    return employee_detail

# Process all employees in parallel:
tasks = [
    process_employee_parallel(session, emp_id, emp_data, ...)
    for emp_id, emp_data in all_employees.items()
]

employee_details = await asyncio.gather(*tasks)
```

**Savings: 2000ms ‚Üí 200ms (10x faster with parallelization)**

---

### **7. Missing Query Optimization - LOW IMPACT**

**What we're doing wrong:**
- Loading entire log objects when only need specific fields
- Not using `.limit()` where applicable
- Getting all columns when only need few

**How much it costs users:**
- **Unnecessary data transfer:** ~100-200ms per request

**How to fix it:**
```python
# BEFORE:
logs = session.exec(
    select(TimeLog).where(...)
).all()

# AFTER - Select only needed columns:
from sqlmodel import col

logs = session.exec(
    select(
        TimeLog.id,
        TimeLog.employee_id, 
        TimeLog.timestamp,
        TimeLog.punch_type,
        TimeLog.dealership_id
    ).where(...)
).all()

# For active check, use limit:
most_recent = session.exec(
    select(TimeLog)
    .where(TimeLog.employee_id == employee_id)
    .order_by(TimeLog.timestamp.desc())
    .limit(1)
).first()
```

**Savings: 100-200ms per request**

---

### **8. No Response Caching - MEDIUM IMPACT**

**What we're doing wrong:**
- Recalculating same data repeatedly
- If 10 users view today's labor report, we calculate it 10 times
- No ETags or cache headers

**How much it costs users:**
- **Redundant calculations:** Full cost for every user viewing same data

**How to fix it:**
```python
from fastapi import Response
from hashlib import md5
import json

# Add response caching
response_cache = {}

@router.get("/labor/daily/enhanced")
async def get_enhanced_daily_labor_spend(
    target_date: date,
    response: Response,
    ...
):
    cache_key = f"enhanced_daily:{target_date}"
    
    # Check cache (5 min TTL)
    if cache_key in response_cache:
        cached_data, cached_time = response_cache[cache_key]
        if (datetime.now() - cached_time).seconds < 300:
            response.headers["X-Cache"] = "HIT"
            return cached_data
    
    # Calculate
    result = ... # existing logic
    
    # Store in cache
    response_cache[cache_key] = (result, datetime.now())
    response.headers["X-Cache"] = "MISS"
    
    return result
```

**Savings: Subsequent requests: 15s ‚Üí 1ms**

---

### **9. Dealership Name Lookups - LOW IMPACT**

**What we're doing wrong:**
- Fetching dealership names from Firestore inside each endpoint (line 809-813)
- Could be cached or included in employee cache

**How much it costs users:**
- **~100-200ms per request**

**How to fix it:**
- Include in the employee cache
- Load once at startup

**Savings: 100-200ms per request**

---

### **10. Inefficient Timestamp Handling - LOW IMPACT**

**What we're doing wrong:**
- Repeatedly checking and converting timezone-naive timestamps (lines 1048-1049, 1216-1218)
- Should be done once at query level

**How much it costs users:**
- **~50-100ms** of CPU time

**How to fix it:**
```python
# Ensure all timestamps are TZ-aware at creation
# Add to TimeLog model or migration
# Then remove all the tzinfo checks
```

**Savings: 50-100ms per request**

---

## **TOTAL IMPACT SUMMARY**

| Issue | Current Cost | Fixed Cost | Savings |
|-------|-------------|-----------|---------|
| 1. N+1 Firestore | 10-20s | 0.1s | **10-20s** |
| 2. Redundant Firestore | 0.5-1s | 0.005s | **0.5-1s** |
| 3. N+1 Active Checks | 3s | 0.05s | **3s** |
| 4. Duplicate Queries | 0.3s | 0s | **0.3s** |
| 5. Nested Loops | 1s | 0.1s | **0.9s** |
| 6. Serial Processing | 2s | 0.2s | **1.8s** |
| 7. Query Optimization | 0.2s | 0.05s | **0.15s** |
| 8. No Caching | Full | 0.001s | **Variable** |
| 9. Dealership Names | 0.2s | 0.01s | **0.19s** |
| 10. Timestamp Handling | 0.1s | 0.01s | **0.09s** |
| **TOTAL** | **17-28s** | **0.5-1s** | **16-27s** |

## **Implementation Priority**

1. **IMMEDIATE (Week 1):** Issues #1, #2, #3 ‚Üí Save 13-24 seconds
2. **SHORT TERM (Week 2):** Issues #4, #5, #8 ‚Üí Save additional 2-3 seconds  
3. **MEDIUM TERM (Week 3-4):** Issues #6, #7, #9, #10 ‚Üí Save additional 1-2 seconds

With these changes, your API will go from **17-28 seconds** to **0.5-1 second** response time! üöÄ

```python
# BEFORE (lines 1034-1036):
for employee_id, logs in employee_logs.items():
    user_details = await get_user_details(employee_id)  # ‚ùå N queries
    hourly_wage = user_details.get("hourly_wage", 0.0)

# AFTER - Batch fetch at start:
# 1. Cache all employee data at startup or use in-memory cache
employee_cache = {}

async def get_all_employees_batch() -> Dict[str, Dict]:
    """Fetch ALL employees once and cache"""
    users_ref = firestore_db.collection("users").where(
        "role", "in", ["employee", "clockOnlyEmployee", "supplier", "owner"]
    ).stream()
    
    cache = {}
    for doc in users_ref:
        data = doc.to_dict()
        cache[doc.id] = {
            "name": data.get("displayName", "Unknown"),
            "hourly_wage": float(data.get("hourlyWage", 0.0)) if data.get("hourlyWage") else 0.0,
            "dealerships": parse_dealerships(data)
        }
    return cache

# At endpoint start:
if not employee_cache or cache_expired:
    employee_cache = await get_all_employees_batch()  # 1 query for all

# Then use cache:
for employee_id, logs in employee_logs.items():
    employee_data = employee_cache.get(employee_id, DEFAULT_EMPLOYEE)
    hourly_wage = employee_data["hourly_wage"]
```

```python
# Add caching layer with Redis or in-memory cache
from functools import lru_cache
from datetime import datetime, timedelta
import asyncio

# Simple in-memory cache with TTL
_employee_cache = None
_employee_cache_time = None
_CACHE_TTL = timedelta(minutes=5)  # Refresh every 5 minutes

async def get_all_employees_cached() -> Dict[str, Dict]:
    global _employee_cache, _employee_cache_time
    
    now = datetime.now()
    if _employee_cache is None or (now - _employee_cache_time) > _CACHE_TTL:
        print("Refreshing employee cache from Firestore...")
        _employee_cache = await fetch_employees_from_firestore()
        _employee_cache_time = now
        print(f"Cached {len(_employee_cache)} employees")
    
    return _employee_cache

# Or use Redis for distributed caching:
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)

async def get_all_employees_redis_cached() -> Dict[str, Dict]:
    cache_key = "employees:all"
    cached = redis_client.get(cache_key)
    
    if cached:
        return json.loads(cached)
    
    employees = await fetch_employees_from_firestore()
    redis_client.setex(cache_key, 300, json.dumps(employees))  # 5 min TTL
    return employees
```

```python
# BEFORE:
for employee_id in dealership_employees:
    is_active, clock_in_time = await is_employee_currently_active(
        session, employee_id, dealership_id
    )  # ‚ùå 100 separate queries

# AFTER - Single batch query:
async def get_all_active_employees_batch(
    session: Session,
    employee_ids: List[str],
    dealership_id: Optional[str] = None
) -> Dict[str, Tuple[bool, Optional[datetime]]]:
    """Get active status for all employees in ONE query"""
    lookback = datetime.now(timezone.utc) - timedelta(days=3)
    
    # Get most recent punch for ALL employees at once
    latest_punches = session.exec(
        select(TimeLog)
        .where(TimeLog.employee_id.in_(employee_ids))
        .where(TimeLog.timestamp >= lookback)
        .order_by(TimeLog.employee_id, TimeLog.timestamp.desc())
    ).all()
    
    # Group by employee and take most recent
    active_status = {}
    for emp_id in employee_ids:
        emp_punches = [p for p in latest_punches if p.employee_id == emp_id]
        if emp_punches:
            latest = emp_punches[0]
            is_active = (latest.punch_type == PunchType.CLOCK_IN and
                        (not dealership_id or latest.dealership_id == dealership_id))
            active_status[emp_id] = (is_active, latest.timestamp if is_active else None)
        else:
            active_status[emp_id] = (False, None)
    
    return active_status

# Usage:
active_statuses = await get_all_active_employees_batch(session, employee_ids, dealership_id)
for employee_id in dealership_employees:
    is_active, clock_in_time = active_statuses[employee_id]
```

```python
# BEFORE:
today_logs = session.exec(
    select(TimeLog).where(TimeLog.timestamp >= start_of_day)...
).all()

week_logs = session.exec(
    select(TimeLog).where(TimeLog.timestamp >= start_of_week)...
).all()

# AFTER - One query, filter in memory:
all_logs = session.exec(
    select(TimeLog)
    .where(TimeLog.employee_id.in_(employee_ids))
    .where(TimeLog.timestamp >= start_of_week)  # Get full range once
    .order_by(TimeLog.timestamp.asc())
).all()

# Filter in Python (fast):
today_logs = [log for log in all_logs if log.timestamp >= start_of_day]
week_logs = all_logs  # Already have it
```

```python
# BEFORE (line 847-858):
for employee_id in dealership_employees:
    employee_today_logs = [
        log for log in today_logs if log.employee_id == employee_id  # ‚ùå O(n)
    ]
    employee_week_logs = [
        log for log in week_logs if log.employee_id == employee_id  # ‚ùå O(n)
    ]

# AFTER - Pre-group with defaultdict:
from collections import defaultdict

# Group once at the start O(n)
employee_today_logs_map = defaultdict(list)
for log in today_logs:
    employee_today_logs_map[log.employee_id].append(log)

employee_week_logs_map = defaultdict(list)
for log in week_logs:
    employee_week_logs_map[log.employee_id].append(log)

# Then O(1) lookups:
for employee_id in dealership_employees:
    employee_today_logs = employee_today_logs_map[employee_id]  # ‚úÖ O(1)
    employee_week_logs = employee_week_logs_map[employee_id]  # ‚úÖ O(1)
```

```python
# BEFORE:
for employee_id, employee_data in all_employees.items():
    # Process employee synchronously
    detail = await process_employee(employee_id)
    employee_details.append(detail)

# AFTER - Parallel processing:
import asyncio

async def process_employee_parallel(session, employee_id, employee_data, ...):
    # All the processing logic
    return employee_detail

# Process all employees in parallel:
tasks = [
    process_employee_parallel(session, emp_id, emp_data, ...)
    for emp_id, emp_data in all_employees.items()
]

employee_details = await asyncio.gather(*tasks)
```

```python
# BEFORE:
logs = session.exec(
    select(TimeLog).where(...)
).all()

# AFTER - Select only needed columns:
from sqlmodel import col

logs = session.exec(
    select(
        TimeLog.id,
        TimeLog.employee_id, 
        TimeLog.timestamp,
        TimeLog.punch_type,
        TimeLog.dealership_id
    ).where(...)
).all()

# For active check, use limit:
most_recent = session.exec(
    select(TimeLog)
    .where(TimeLog.employee_id == employee_id)
    .order_by(TimeLog.timestamp.desc())
    .limit(1)
).first()
```

```python
from fastapi import Response
from hashlib import md5
import json

# Add response caching
response_cache = {}

@router.get("/labor/daily/enhanced")
async def get_enhanced_daily_labor_spend(
    target_date: date,
    response: Response,
    ...
):
    cache_key = f"enhanced_daily:{target_date}"
    
    # Check cache (5 min TTL)
    if cache_key in response_cache:
        cached_data, cached_time = response_cache[cache_key]
        if (datetime.now() - cached_time).seconds < 300:
            response.headers["X-Cache"] = "HIT"
            return cached_data
    
    # Calculate
    result = ... # existing logic
    
    # Store in cache
    response_cache[cache_key] = (result, datetime.now())
    response.headers["X-Cache"] = "MISS"
    
    return result
```

```python
# Ensure all timestamps are TZ-aware at creation
# Add to TimeLog model or migration
# Then remove all the tzinfo checks
```

